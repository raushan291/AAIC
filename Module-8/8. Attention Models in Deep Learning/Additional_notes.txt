Additional notes:



1. We mentioned that Tx is a hyperparam, which is actually explained in a 2015 paper [ https://arxiv.org/pdf/1508.04025.pdf ] and not in the original 2014 Attention Models paper [https://arxiv.org/pdf/1409.0473.pdf]  that we referred to in the above session. In the 2014 paper, Tx is the length of the whole input sentence.



2. We missed out one more minor equation which explains how S0 is computed in the decoder. S0 is the input left most input in the Decoder and its value is tanh(Ws * h1_backward_arrow). where Ws is a weight matrix and h1_backward arrow is the value of the output of the left most LSTM in the bi-directional LSTM in the encoder. The tanh() & Ws are simply equalivant to having a tanh activation fully-connected layer.